<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Assistant Vocal iPhone Style Ultra-Rapide</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <script src="https://unpkg.com/lucide@latest/dist/umd/lucide.js"></script>
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    fontFamily: {
                        sans: ['Inter', '-apple-system', 'BlinkMacSystemFont', 'Segoe UI', 'Roboto', 'sans-serif'],
                    },
                    borderRadius: {
                        '4xl': '2rem',
                        '5xl': '2.5rem',
                    },
                    keyframes: {
                        pulse: {
                            '0%, 100%': { boxShadow: '0 0 0 0 rgba(74, 222, 128, 0.7)' },
                            '70%': { boxShadow: '0 0 0 10px rgba(74, 222, 128, 0)' },
                        },
                        'tts-pulse': {
                            '0%, 100%': { boxShadow: '0 0 0 0 rgba(168, 85, 247, 0.7)' },
                            '70%': { boxShadow: '0 0 0 10px rgba(168, 85, 247, 0)' },
                        },
                        'interrupt-flash': {
                            '0%': { boxShadow: '0 0 0 0 rgba(239, 68, 68, 0.8)' },
                            '50%': { boxShadow: '0 0 0 15px rgba(239, 68, 68, 0.4)' },
                            '100%': { boxShadow: '0 0 0 0 rgba(239, 68, 68, 0)' },
                        },
                        'processing-spin': {
                            '0%': { transform: 'rotate(0deg)' },
                            '100%': { transform: 'rotate(360deg)' },
                        },
                    },
                    animation: {
                        pulse: 'pulse 2s infinite',
                        'tts-pulse': 'tts-pulse 1.5s infinite',
                        'interrupt-flash': 'interrupt-flash 0.5s ease-out',
                        'processing-spin': 'processing-spin 1s linear infinite',
                    },
                },
            },
        }
    </script>
    <style>
        ::-webkit-scrollbar { width: 4px; }
        ::-webkit-scrollbar-track { background: rgba(255,255,255,0.1); border-radius: 10px; }
        ::-webkit-scrollbar-thumb { background: rgba(255,255,255,0.3); border-radius: 10px; }
        
        #toggleButton {
    transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
    transform-origin: center;
}

#toggleButton:active {
    transform: scale(0.95);
}

#toggleButton:disabled {
    transform: scale(1);
    opacity: 0.5;
}
        .ultra-fast-indicator {
            position: absolute;
            top: 10px;
            right: 10px;
            background: linear-gradient(45deg, #00ff00, #ffff00);
            color: black;
            padding: 4px 8px;
            border-radius: 12px;
            font-size: 10px;
            font-weight: bold;
            animation: pulse 2s infinite;
        }
    </style>
</head>
<!-- <body class="bg-gradient-to-br from-[#667eea] to-[#764ba2] min-h-screen flex items-center justify-center font-sans"> -->
    <body class="bg-gradient-to-br from-[#1e3a8a] to-[#3730a3] min-h-screen flex items-center justify-center font-sans">

    <div class="w-full h-full relative">
        <!-- <div class="ultra-fast-indicator">‚ö° ULTRA-RAPIDE</div> -->
        
        <!-- <div class="w-full h-full bg-gradient-to-br from-[#1e3c72] to-[#2a5298] relative overflow-hidden flex flex-col"> -->
        <!-- Remplace cette section dans ton HTML : -->
<div class="fixed top-[0px] left-0 w-full z-10">
    <div class="flex justify-between items-center px-5 py-3 text-white text-sm font-semibold">
        <div>9:41</div>
        <!-- <div>Assistant Vocal Pro</div> -->
        <div id="nav-menu-toggle">
            <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16" />
            </svg>
        </div>
        <div class="flex items-center gap-2">
            <!-- ‚ú® NOUVEAU BOUTON DRAWER -->
            <button id="drawerToggle" class="p-2 rounded-lg bg-white/10 hover:bg-white/20 transition-colors">
                <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16" />
                </svg>
            </button>
            <button id="conversationToggle" class="p-2 rounded-lg bg-white/10 hover:bg-white/20">
                üí¨
            </button>
        </div>
    </div>
</div>
            <div class="px-5 flex-1 flex flex-col min-h-0 mt-6">
                <div class="hidden bg-white/10 backdrop-blur-xl rounded-2xl border border-white/20 flex-shrink-0 my-5">
                    <div class="flex items-center justify-between p-3 cursor-pointer" id="settingsToggle">
                        <div class="flex items-center gap-2">
                            <span class="text-white text-sm font-medium">‚öôÔ∏è Param√®tres</span>
                        </div>
        


                        <span id="toggleIcon" class="text-white/70 transition-transform duration-300">‚¨áÔ∏è</span>
                    </div>
                    
                    <div id="settingsContent" class="px-5 pb-5 transition-all duration-300 ease-in-out">
                        <input type="text" id="groqApiKey" placeholder="Cl√© API Groq (STT)" class="w-full bg-white/10 border border-white/20 rounded-xl px-4 py-3 text-white text-sm placeholder-white/60 focus:outline-none focus:border-green-400/50 focus:bg-white/15 mb-3">
                        <input type="text" id="openaiApiKey" placeholder="Cl√© API OpenAI (LLM)" class="w-full bg-white/10 border border-white/20 rounded-xl px-4 py-3 text-white text-sm placeholder-white/60 focus:outline-none focus:border-green-400/50 focus:bg-white/15">
                        
                        <div class="flex gap-2 items-center mt-3 text-xs">
                            <div class="flex items-center gap-1">
                                <span class="text-white">Auto TTS</span>
                                <label class="relative inline-block w-[40px] h-[24px]">
                                    <input type="checkbox" id="autoSpeak" class="opacity-0 w-0 h-0 peer" checked>
                                    <span class="absolute cursor-pointer top-0 left-0 right-0 bottom-0 bg-white/30 rounded-full transition-colors peer-checked:bg-green-400 before:absolute before:content-[''] before:h-[18px] before:w-[18px] before:left-[3px] before:bottom-[3px] before:bg-white before:rounded-full before:transition-transform peer-checked:before:translate-x-4"></span>
                                </label>
                            </div>
                            <select id="voiceSelect" class="bg-white/10 border border-white/20 rounded-lg px-2 py-1 text-white text-xs appearance-none">
                                <option value="fr-FR-DeniseNeural">Denise</option>
                                <option value="fr-FR-EloiseNeural">Eloise</option>
                                <option value="fr-FR-FabriceNeural">Fabrice</option>
                                <option value="fr-FR-HenriNeural">Henri</option>
                            </select>
                            <div class="flex items-center gap-1">
                                <label class="relative inline-block w-[40px] h-[24px]">
                                    <input type="checkbox" id="allowInterrupt" class="opacity-0 w-0 h-0 peer" checked>
                                    <span class="absolute cursor-pointer top-0 left-0 right-0 bottom-0 bg-white/30 rounded-full transition-colors peer-checked:bg-green-400 before:absolute before:content-[''] before:h-[18px] before:w-[18px] before:left-[3px] before:bottom-[3px] before:bg-white before:rounded-full before:transition-transform peer-checked:before:translate-x-4"></span>
                                </label>
                                <span class="text-white">Interrupt</span>
                            </div>
                        </div>
                        
                        <div class="mt-3 flex justify-between text-xs text-white/60">
                            <div>Cache: <span id="cache-hits" class="text-green-400">0</span></div>
                            <div>Latence: <span id="latency" class="text-blue-400">0ms</span></div>
                            <div>Qualit√©: <span id="quality" class="text-yellow-400">0%</span></div>
                        </div>
                    </div>
                </div>
                
                <div class="relative flex-shrink-0 mt-[-200px]">
                    <div id="voice-circle" class="w-[200px] h-[200px] rounded-full mx-auto mt-10 mb-2.5 flex items-center justify-center text-6xl transition-all duration-300 bg-white/10 backdrop-blur-xl border-2 border-white/20">
                        <i data-lucide="moon" class="w-16 h-16"></i>
                    </div>

                    <div id="status-text" class="text-center text-white text-lg font-medium">En attente...</div>
                    <div id="interrupt-indicator" class="absolute top-[220px] left-1/2 -translate-x-1/2 bg-red-500/80 text-white px-4 py-2 rounded-full text-xs opacity-0 transition-opacity duration-300">Interruption d√©tect√©e!</div>
                    <div id="processing-indicator" class="absolute top-[250px] left-1/2 -translate-x-1/2 bg-blue-500/80 text-white px-4 py-2 rounded-full text-xs opacity-0 transition-opacity duration-300">‚ö° Traitement ultra-rapide...</div>
                </div>
                
                <div id="conversation-area" class="flex-1  overflow-y-auto min-h-0 my-2 p-4 rounded-2xl bg-white/5 backdrop-blur-xl border border-white/10" style="display: block;">
                    <div id="conversation">
                        <div class="text-white/60 text-center italic mt-10">
                            Appuyez sur d√©marrer ou envoyez un message...<br>
                            <small class="mt-2 block">‚ö° Mode Ultra-Rapide activ√©!</small>
                        </div>
                    </div>
                </div>

                <div class="fixed bottom-0 left-0 w-full z-6 flex-shrink-0 my-2">
                    <div class=" gap-3 mb-3">
                     <div class="flex justify-center items-center">

                         <!-- NOUVEAU BOUTON TOGGLE UNIQUE -->
                         <button id="toggleButton" class="flex-1 py-4 rounded-2xl font-semibold text-base transition-all backdrop-blur-xl bg-green-500/80 text-white disabled:opacity-50 disabled:cursor-not-allowed flex items-center justify-center w-80 gap-2">
                             <i data-lucide="mic" id="toggleIcon"></i>
                             <span id="toggleText">D√©marrer</span>
                            </button>
                        </div> 
                            <div class="flex justify-center items-center mt-4 gap-2 ">

                            <button id="modeContinu" class="flex-1 py-2 px-3 rounded-lg text-sm font-medium transition-all bg-green-500/80 text-white">
                                üîÑ Continu
                            </button>
                            <button id="modePushToTalk" class="flex-1 py-2 px-3 rounded-lg text-sm font-medium transition-all bg-white/20 text-white/70 hover:bg-white/30">
                                üé§ Push-to-Talk
                            </button>
                        </div>
                      
                        <!-- <button id="clearButton" class="flex-1 py-4 rounded-2xl font-semibold text-base transition-all backdrop-blur-xl bg-white/20 text-white border border-white/30 disabled:opacity-50 disabled:cursor-not-allowed flex items-center justify-center gap-2">
                            <i data-lucide="trash-2"></i>
                        </button> -->
                    
                    </div>
                    <div class="flex gap-2 mb-2">
                        <input type="text" id="text-input" placeholder="Envoyer un message..." class="flex-1 bg-white/10 border border-white/20 rounded-xl px-4 py-3 text-white text-sm placeholder-white/60 focus:outline-none focus:border-green-400/50 focus:bg-white/15">
                        <button id="send-button" class="p-3 bg-green-500/80 rounded-xl text-white hover:bg-green-500/100 transition-colors">
                            <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor" stroke-width="2">
                                <path stroke-linecap="round" stroke-linejoin="round" d="M12 19l9 2-9-18-9 18 9-2zm0 0v-8" />
                            </svg>
                        </button>
                    </div>
                    
                </div>
<!--                 
                <div id="debug" class="flex-shrink-0 bg-black/20 rounded-xl p-2.5 mb-2 text-xs text-white/70 h-[60px] overflow-y-auto">
                    <div>‚ö° Syst√®me ultra-rapide pr√™t...</div>
                </div> -->
            </div>
        </div>
    </div>

    <audio id="ttsAudio" style="display: none;" preload="none"></audio>

    <script defer src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.14.0/dist/ort.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/@ricky0123/vad-web@0.0.22/dist/bundle.min.js"></script>
    <script defer src="https://unpkg.com/@cartesia/cartesia-js@0.1.7/dist/bundle.min.js"></script>

    
    <script>
        // =============================================
        // üöÄ ASSISTANT VOCAL ULTRA-RAPIDE COMPLET
        // =============================================
        // Variables pour le bouton toggle
let isRecording = false;
let toggleButton, toggleIcon, toggleText;

let cartesiaClient; // On va l'initialiser plus tard

        // üîß VARIABLES GLOBALES
        let isSettingsExpanded = true;
        let myVad = null;
        let conversationHistory = [];
        let isPlaying = false;
        let isProcessingTTS = false;
        let currentAudioUrl = null;
        let pipelineActive = false;
        let generationWorkers = [];
        let readyAudioBuffer = new Map();


        let lastPlayedReflection = ""; // NOUVELLE LIGNE √Ä AJOUTER
        // üÜï NOUVEAU : Variables pour les modes d'enregistrement


        // AJOUTER ICI apr√®s lastPlayedReflection
let phrasePipeline = [];
let currentPhraseIndex = 0;
let phraseGenerationActive = false;
let mainResponseReady = false;
let reflectionAudio = null;
let recordingMode = 'continu'; // 'continu' ou 'push-to-talk'
let isPushToTalkRecording = false;
let pushToTalkAudio = [];
let modeContinu, modePushToTalk;
        const transcriptionCache = new Map();
        const ttsCache = new Map();
        const MAX_CACHE_SIZE = 50;
        const MAX_BUFFER_SIZE = 3;
        const baseCircleClasses = "w-[200px] h-[200px] rounded-full mx-auto mt-10 mb-2.5 flex items-center justify-center text-6xl transition-all duration-300 backdrop-blur-xl";
        
        // üîß √âL√âMENTS DOM
        let groqApiKeyInput, openaiApiKeyInput, startButton, stopButton, clearButton;
        let voiceCircle, statusText, debug, conversation, autoSpeakCheckbox;
        let allowInterruptCheckbox, voiceSelect, ttsAudio, interruptIndicator;
        let processingIndicator, cacheHitsElement, latencyElement, qualityElement;
        let textInput, sendButton;

// üí≠ NOUVELLES VARIABLES R√âFLEXIONS SIMPLES
let reflectionAudios = new Map();
let isPlayingReflection = false;

// Liste simple de r√©flexions sans analyse
const SIMPLE_REFLECTIONS = [
"Ah oui, je vois...",
"D'accord, tr√®s bien r√©mi, laisse moi le temps de reflechir...",
"OK,ok  je comprends, r√©mi je te comprends ...",

];

        // AJOUTEZ CETTE LIGNE √Ä LA FIN
    lucide.createIcons();

log('üé® Ic√¥nes Lucide initialis√©es.', 'success');


        // =============================================
        // üîß FONCTIONS UTILITAIRES
        // =============================================
        
        function log(message, type = 'info') {
            const timestamp = new Date().toLocaleTimeString();
            const emoji = type === 'success' ? '‚úÖ' : type === 'error' ? '‚ùå' : type === 'warning' ? '‚ö†Ô∏è' : '‚ö°';
            const logMessage = `${timestamp} ${emoji} ${message}`;
            
            console.log(logMessage);
            if (debug) {
                const newLine = document.createElement('div');
                newLine.textContent = logMessage;
                newLine.className = type === 'error' ? 'text-red-300' : type === 'success' ? 'text-green-300' : type === 'warning' ? 'text-yellow-300' : '';
                debug.insertBefore(newLine, debug.firstChild);
                
                while (debug.children.length > 10) {
                    debug.removeChild(debug.lastChild);
                }
            }
        }
        
        function toggleSettings() {
            const content = document.getElementById('settingsContent');
            const icon = document.getElementById('toggleIcon');
            
            isSettingsExpanded = !isSettingsExpanded;
            if (isSettingsExpanded) {
                content.style.maxHeight = '500px'; 
                content.style.paddingBottom = '20px';
                content.style.opacity = '1';
                icon.textContent = '‚¨áÔ∏è';
                log('Param√®tres d√©ploy√©s', 'info');
            } else {
                content.style.maxHeight = '0px';
                content.style.paddingBottom = '0px';
                content.style.opacity = '0';
                icon.textContent = '‚û°Ô∏è';
                log('Param√®tres r√©tract√©s', 'info');
            }
        }
        
        function updateStatus(text, emoji, stateClass) {
    if (!statusText || !voiceCircle) return;
    
    statusText.textContent = text;

    // Modifiez cette partie
    let iconName = 'pause-circle'; // Ic√¥ne par d√©faut
    switch (stateClass) {
        case "listening": iconName = 'mic'; break;
        case "speaking": iconName = 'volume-2'; break;
        case "processing": iconName = 'loader-circle'; break;
        case "tts-playing": iconName = 'volume-2'; break; // On peut utiliser la m√™me que speaking
        case "interrupted": iconName = 'zap-off'; break;
        default: iconName = 'power-off'; // En attente -> √©teint
    }
    voiceCircle.innerHTML = `<i data-lucide="${iconName}" class="w-16 h-16"></i>`;
    
    processingIndicator?.classList.remove('opacity-100');
    
    let stateClasses = "";
    switch (stateClass) {
        case "listening": stateClasses = "bg-green-400/20 border-2 border-green-400/40 animate-pulse"; break;
        case "speaking": stateClasses = "bg-green-600/30 border-2 border-green-600/50"; break;
        case "processing":
            stateClasses = "bg-amber-400/20 border-2 border-amber-400/40";
            // On garde l'ic√¥ne de chargement mais on peut enlever le spin CSS si l'ic√¥ne tourne d√©j√†
            voiceCircle.innerHTML = `<i data-lucide="loader-circle" class="w-16 h-16 animate-spin"></i>`;
            processingIndicator?.classList.add('opacity-100');
            break;
        case "tts-playing": stateClasses = "bg-purple-500/20 border-2 border-purple-500/40 animate-tts-pulse"; break;
        case "interrupted": stateClasses = "bg-red-500/20 border-2 border-red-500/40 animate-interrupt-flash"; break;
        default: stateClasses = "bg-white/10 border-2 border-white/20";
    }
    voiceCircle.className = `${baseCircleClasses} ${stateClasses}`;

    // IMPORTANT : Mettre √† jour les ic√¥nes apr√®s chaque changement
    lucide.createIcons();
}
        // =============================================
        // üîß FONCTIONS AUDIO
        // =============================================
        
        function analyzeAudioQuality(audioData) { 
            const energy = audioData.reduce((sum, sample) => sum + sample * sample, 0) / audioData.length; 
            if (energy < 0.0008) { log('Audio trop silencieux, ignor√©', 'warning'); return { quality: 0, pass: false }; } 
            const mean = audioData.reduce((sum, sample) => sum + sample, 0) / audioData.length; 
            const variance = audioData.reduce((sum, sample) => sum + Math.pow(sample - mean, 2), 0) / audioData.length; 
            if (variance < 0.00005) { log('Audio monotone d√©tect√©, ignor√©', 'warning'); return { quality: 0, pass: false }; } 
            const quality = Math.min(100, Math.floor((energy * 10000) + (variance * 100000))); 
            return { quality, pass: quality > 15 }; 
        }


        // üî• AJOUTEZ CETTE FONCTION ICI
function isLikelyBackgroundNoise(audioData) {
    // D√©tecter si c'est du bruit de fond (TV, radio, etc.)
    const energy = audioData.reduce((sum, sample) => sum + sample * sample, 0) / audioData.length;
    const maxAmplitude = Math.max(...audioData.map(Math.abs));
    
    // Si l'√©nergie est faible mais constante, c'est probablement du bruit de fond
    if (energy > 0.0005 && energy < 0.002 && maxAmplitude < 0.3) {
        return true;
    }
    return false;
}

        function compressAudio(audioData, targetSampleRate = 8000) { 
            const maxSamples = targetSampleRate * 15; 
            if (audioData.length > maxSamples) { 
                const ratio = Math.ceil(audioData.length / maxSamples); 
                const compressed = []; 
                for (let i = 0; i < audioData.length; i += ratio) { 
                    compressed.push(audioData[i]); 
                } 
                log(`Audio compress√©: ${audioData.length} ‚Üí ${compressed.length} √©chantillons`, 'success'); 
                return new Float32Array(compressed); 
            } 
            return audioData; 
        }

        function hashAudio(audioData) { 
            const sampleSize = Math.min(500, audioData.length); 
            const step = Math.max(1, Math.floor(audioData.length / sampleSize)); 
            let hash = 0; 
            for (let i = 0; i < audioData.length; i += step) { 
                const sample = Math.floor((audioData[i] + 1) * 127.5); 
                hash = ((hash << 5) - hash + sample) & 0xffffffff; 
            } 
            return hash.toString(36); 
        }

        function getCachedTranscription(audioHash) { 
            if (transcriptionCache.has(audioHash)) { 
                return transcriptionCache.get(audioHash); 
            } 
            return null; 
        }

        function setCachedTranscription(audioHash, transcription) { 
            if (transcriptionCache.size >= MAX_CACHE_SIZE) { 
                const firstKey = transcriptionCache.keys().next().value; 
                transcriptionCache.delete(firstKey); 
            } 
            transcriptionCache.set(audioHash, transcription); 
        }

        function updateLatency(latency) { 
            if (latencyElement) {
                latencyElement.textContent = `${latency}ms`; 
                latencyElement.className = latency < 2000 ? 'text-green-400' : 'text-yellow-400'; 
            }
        }

        function updateQuality(quality) { 
            if (qualityElement) {
                qualityElement.textContent = `${quality}%`; 
                qualityElement.className = quality > 70 ? 'text-green-400' : 'text-yellow-400'; 
            }
        }

        function encodeWAV(samples, sampleRate = 16000) { 
            const buffer = new ArrayBuffer(44 + samples.length * 2); 
            const view = new DataView(buffer); 
            const writeString = (offset, string) => { 
                for (let i = 0; i < string.length; i++) view.setUint8(offset + i, string.charCodeAt(i)); 
            }; 
            writeString(0, 'RIFF'); 
            view.setUint32(4, 36 + samples.length * 2, true); 
            writeString(8, 'WAVE'); 
            writeString(12, 'fmt '); 
            view.setUint32(16, 16, true); 
            view.setUint16(20, 1, true); 
            view.setUint16(22, 1, true); 
            view.setUint32(24, sampleRate, true); 
            view.setUint32(28, sampleRate * 2, true); 
            view.setUint16(32, 2, true); 
            view.setUint16(34, 16, true); 
            writeString(36, 'data'); 
            view.setUint32(40, samples.length * 2, true); 
            let offset = 44; 
            for (let i = 0; i < samples.length; i++, offset += 2) { 
                view.setInt16(offset, Math.max(-1, Math.min(1, samples[i])) * 0x7FFF, true); 
            } 
            return buffer; 
        }
// =============================================
// üí≠ SYST√àME DE R√âFLEXIONS SIMPLES
// =============================================
async function preGenerateSimpleReflections() {
log('üí≠ Pr√©-g√©n√©ration des r√©flexions simples...', 'info');

for (const phrase of SIMPLE_REFLECTIONS) {
    try {
        const audioUrl = await generateTTSForSentence(phrase, voiceSelect.value, 0);
        reflectionAudios.set(phrase, audioUrl);
        log(`‚úÖ R√©flexion: "${phrase}"`, 'success');
    } catch (error) {
        log(`‚ùå Erreur r√©flexion: ${phrase}`, 'error');
    }
}

log(`üéâ ${reflectionAudios.size} r√©flexions simples pr√©-g√©n√©r√©es !`, 'success');
}

// AJOUTER TOUTES CES FONCTIONS ICI
async function playRandomReflectionAsync() {
    if (isPlayingReflection || reflectionAudios.size === 0) return null;
    
    const randomReflection = SIMPLE_REFLECTIONS[Math.floor(Math.random() * SIMPLE_REFLECTIONS.length)];
    const audioUrl = reflectionAudios.get(randomReflection);
    
    if (audioUrl) {
        isPlayingReflection = true;
        lastPlayedReflection = randomReflection;
        
        reflectionAudio = new Audio(audioUrl);
        
        return new Promise((resolve) => {
            reflectionAudio.onended = () => {
                isPlayingReflection = false;
                log('üí≠ R√©flexion termin√©e - pr√™t pour encha√Ænement', 'info');
                resolve();
            };
            
            reflectionAudio.onerror = () => {
                isPlayingReflection = false;
                log('‚ùå Erreur r√©flexion', 'error');
                resolve();
            };
            
            reflectionAudio.play();
            log(`üí≠ R√©flexion: "${randomReflection}"`, 'success');
        });
    }
    return null;
}

async function processUserMessageParallel(audio) {
    try {
        const userText = await transcribeAudioFromBuffer(audio);
        if (!userText || userText.length < 3) return null;
        
        log(`Message re√ßu: "${userText}"`, 'info');
        addToConversation('user', userText);
        
        const aiResponse = await getLLMResponseStreaming(userText);
        if (aiResponse) {
            addToConversation('assistant', aiResponse);
            
            // üî• D√âMARRER G√âN√âRATION TTS PHRASE PAR PHRASE IMM√âDIATEMENT
            await startPhrasePipelineGeneration(aiResponse);
        }
        
        return { userText, aiResponse };
        
    } catch (error) {
        log(`Erreur traitement parall√®le: ${error.message}`, 'error');
        return null;
    }
}

async function transcribeAudioFromBuffer(audioData) {
    const startTime = performance.now();
    const groqKey = groqApiKeyInput?.value?.trim();
    
    if (!groqKey) return null;
    
    try {
        const qualityResult = analyzeAudioQuality(audioData);
        updateQuality(qualityResult.quality);
        if (!qualityResult.pass) return null;
        
        updateStatus("Transcription...", "‚ö°", "processing");
        const compressedAudio = compressAudio(audioData);
        const wavBuffer = encodeWAV(compressedAudio);
        const formData = new FormData();
        formData.append('file', new Blob([wavBuffer], { type: 'audio/wav' }), 'audio.wav');
        formData.append('model', 'whisper-large-v3');
        formData.append('response_format', 'json');
        formData.append('language', 'fr');
        
        const controller = new AbortController();
        setTimeout(() => controller.abort(), 10000);
        
        const transResponse = await fetch('https://api.groq.com/openai/v1/audio/transcriptions', {
            method: 'POST',
            headers: { 'Authorization': `Bearer ${groqKey}` },
            body: formData,
            signal: controller.signal
        });
        
        if (!transResponse.ok) throw new Error(`Transcription √©chou√©e: ${transResponse.status}`);
        const transResult = await transResponse.json();
        let userText = transResult.text?.trim();
        
        // Filtres anti-parasite
        const parasiteFilters = [
            /sous[\-\s]*titrage/i,
            /soci√©t√©[\-\s]*radio[\-\s]*canada/i,
            /radio[\-\s]*canada/i,
            /src[\-\s]*subtitle/i
        ];

        for (const filter of parasiteFilters) {
            if (filter.test(userText)) {
                log('Texte parasite d√©tect√© et ignor√©: ' + userText, 'warning');
                return null;
            }
        }
        
        updateLatency(Math.round(performance.now() - startTime));
        return userText;
        
    } catch (error) {
        log(`Erreur transcription: ${error.message}`, 'error');
        return null;
    }
}

async function startPhrasePipelineGeneration(aiResponse) {
    phraseGenerationActive = true;
    const sentences = splitIntoSentences(aiResponse);
    phrasePipeline = new Array(sentences.length);
    currentPhraseIndex = 0;
    
    log(`üîÑ G√©n√©ration pipeline: ${sentences.length} phrases`, 'info');
    
    // G√©n√©rer toutes les phrases en parall√®le
    const generationPromises = sentences.map(async (sentence, i) => {
        if (!phraseGenerationActive) return;
        
        try {
            const audioUrl = await generateTTSForSentence(sentence, voiceSelect.value, i);
            phrasePipeline[i] = {
                audioUrl: audioUrl,
                sentence: sentence,
                ready: true
            };
            log(`‚úÖ Phrase ${i} pr√™te: "${sentence.substring(0, 30)}..."`, 'success');
        } catch (error) {
            log(`‚ùå Erreur phrase ${i}`, 'error');
            phrasePipeline[i] = { ready: false };
        }
    });
    
    mainResponseReady = true;
    
    // Attendre que toutes les phrases soient trait√©es (en arri√®re-plan)
    Promise.allSettled(generationPromises);
}

async function playMainResponseStreamingStyle() {
    updateStatus("IA parle...", "üîä", "tts-playing");
    currentPhraseIndex = 0;
    
    while (currentPhraseIndex < phrasePipeline.length && phraseGenerationActive) {
        // Attendre que la phrase courante soit pr√™te
        while (!phrasePipeline[currentPhraseIndex]?.ready && phraseGenerationActive) {
            await new Promise(resolve => setTimeout(resolve, 50));
        }
        
        const phraseData = phrasePipeline[currentPhraseIndex];
        if (phraseData && phraseData.ready && phraseData.audioUrl) {
            
            log(`üîä Phrase ${currentPhraseIndex}: "${phraseData.sentence.substring(0, 30)}..."`, 'info');
            
            if (currentAudioUrl) {
                URL.revokeObjectURL(currentAudioUrl);
            }
            currentAudioUrl = phraseData.audioUrl;
            ttsAudio.src = phraseData.audioUrl;
            
            await new Promise((resolve) => {
                ttsAudio.onended = () => {
                    log(`‚úÖ Phrase ${currentPhraseIndex} termin√©e`, 'success');
                    resolve();
                };
                ttsAudio.onerror = () => {
                    log(`‚ùå Erreur lecture phrase ${currentPhraseIndex}`, 'error');
                    resolve();
                };
                ttsAudio.play();
            });
        }
        
        currentPhraseIndex++;
    }
    
    cleanupPhrasePipeline();
    if (myVad) updateStatus("En √©coute...", "üëÇ", "listening");
}


async function handleParallelPipeline(reflectionPromise, processingPromise) {
    // üî• D√âMARRER IMM√âDIATEMENT LA R√âFLEXION (0ms)
    if (reflectionPromise) {
        reflectionPromise; // Pas d'await - joue imm√©diatement !
        log('üéØ R√©flexion IMM√âDIATE lanc√©e', 'success');
    }
    
    // üî• D√âMARRER LE PROCESSING EN PARALL√àLE TOTAL
    processingPromise;
    
    // üî• ATTENDRE MOINS LONGTEMPS (5ms au lieu de 20ms)
    let attempts = 0;
    while (!phrasePipeline[0]?.ready && attempts < 25) { // 25 x 5ms = 125ms max
        await new Promise(resolve => setTimeout(resolve, 5)); // Plus rapide !
        attempts++;
    }
    
    // üî• D√âMARRER TTS M√äME SI PAS PR√äT (risqu√© mais ultra-rapide)
    playMainResponseStreamingStyle();
}


function cleanupPhrasePipeline() {
    phraseGenerationActive = false;
    
    // Nettoyer les URLs
    phrasePipeline.forEach(phrase => {
        if (phrase?.audioUrl) {
            URL.revokeObjectURL(phrase.audioUrl);
        }
    });
    
    phrasePipeline = [];
    currentPhraseIndex = 0;
    mainResponseReady = false;
    
    log('üßπ Pipeline phrases nettoy√©', 'info');
}


        function splitIntoSentences(text) {
            const sentences = text
                .replace(/([.!?])\s+/g, '$1|SPLIT|')
                .replace(/([.!?])$/g, '$1|SPLIT|')
                .split('|SPLIT|')
                .map(s => s.trim())
                .filter(s => s.length > 3);
            
            log(`Texte d√©coup√© en ${sentences.length} phrases pour pipeline`, 'info');
            return sentences;
        }

        function hashText(text) {
            let hash = 0;
            for (let i = 0; i < text.length; i++) {
                const char = text.charCodeAt(i);
                hash = ((hash << 5) - hash) + char;
                hash = hash & hash;
            }
            return hash.toString(36);
        }

        async function generateTTSForSentence(sentence, voice, sentenceIndex) {
    // Cl√© API Cartesia - remplacez par la v√¥tre
    const CARTESIA_API_KEY = "sk_car_j18nkdGqMkXxdvYwPJVJiJ";
    
    // ID de la voix "Antoine" (ou choisissez une autre voix)
    const VOICE_ID = "0b1380da-611b-4d00-83f4-8a969a53e4e0";
    
    const cacheKey = `${hashText(sentence)}_${VOICE_ID}`;
    
    // V√©rifier le cache
    if (ttsCache.has(cacheKey)) {
        log(`TTS cache hit pour phrase ${sentenceIndex}`, 'success');
        return ttsCache.get(cacheKey);
    }

    try {
        log(`üîÑ G√©n√©ration TTS Cartesia phrase ${sentenceIndex}: "${sentence.substring(0, 30)}..."`, 'info');
        
        const response = await fetch("https://api.cartesia.ai/tts/bytes", {
            method: "POST",
            headers: {
                "Cartesia-Version": "2024-06-10",
                "X-API-Key": CARTESIA_API_KEY,
                "Content-Type": "application/json",
            },
            body: JSON.stringify({
                model_id: "sonic-2",
                transcript: sentence,
                voice: {
                    mode: "id",
                    id: VOICE_ID
                },
                output_format: {
                    container: "raw",
                    encoding: "pcm_f32le",
                    sample_rate: 24000
                },
                language: "fr"
            }),
        });

        if (!response.ok) {
            throw new Error(`HTTP ${response.status}: ${await response.text()}`);
        }
        
        const rawAudioBuffer = await response.arrayBuffer();
        const audioFloat32 = new Float32Array(rawAudioBuffer);
        const wavBuffer = encodeWAV(audioFloat32, 24000);
        const audioBlob = new Blob([wavBuffer], { type: 'audio/wav' });
        const audioUrl = URL.createObjectURL(audioBlob);
        
        // Mettre en cache
        ttsCache.set(cacheKey, audioUrl);
        
        log(`‚úÖ TTS Cartesia pr√™t pour phrase ${sentenceIndex}`, 'success');
        return audioUrl;

    } catch (error) {
        log(`‚ùå Erreur TTS Cartesia phrase ${sentenceIndex}: ${error.message}`, 'error');
        throw error;
    }
}
async function pipelineWorker(sentences, voice, startIndex = 0) {
            for (let i = startIndex; i < sentences.length && pipelineActive; i++) {
                while (readyAudioBuffer.size >= MAX_BUFFER_SIZE && pipelineActive) {
                    await new Promise(resolve => setTimeout(resolve, 100));
                }

                if (!pipelineActive) break;

                try {
                    const audioUrl = await generateTTSForSentence(sentences[i], voice, i);
                    if (pipelineActive) {
                        readyAudioBuffer.set(i, {
                            audioUrl: audioUrl,
                            sentence: sentences[i],
                            index: i
                        });
                        log(`üì¶ Phrase ${i} ajout√©e au buffer (taille: ${readyAudioBuffer.size})`, 'info');
                    }
                } catch (error) {
                    log(`Erreur g√©n√©ration phrase ${i}, ignor√©e`, 'error');
                }
            }
            log('üèÅ Pipeline worker termin√©', 'info');
        }

        async function playAudioPipeline(sentences, voice) {
            let currentIndex = 0;
            
            while (currentIndex < sentences.length && pipelineActive) {
                while (!readyAudioBuffer.has(currentIndex) && pipelineActive) {
                    await new Promise(resolve => setTimeout(resolve, 50));
                }

                if (!pipelineActive) break;

                const audioData = readyAudioBuffer.get(currentIndex);
                readyAudioBuffer.delete(currentIndex);

                try {
                    updateStatus("IA parle...", "üîä", "tts-playing");
                    
                    if (currentAudioUrl) {
                        URL.revokeObjectURL(currentAudioUrl);
                    }
                    currentAudioUrl = audioData.audioUrl;

                    ttsAudio.src = audioData.audioUrl;

                    await new Promise((resolve, reject) => {
                        if (!pipelineActive) {
                            resolve();
                            return;
                        }

                        ttsAudio.onloadeddata = () => {
                            if (pipelineActive) {
                                ttsAudio.play().then(() => {
                                    log(`üîä Lecture phrase ${currentIndex}: "${audioData.sentence.substring(0, 30)}..."`, 'info');
                                }).catch(reject);
                            } else {
                                resolve();
                            }
                        };

                        ttsAudio.onended = () => {
                            log(`‚úÖ Phrase ${currentIndex} termin√©e`, 'success');
                            resolve();
                        };

                        ttsAudio.onerror = () => {
                            log(`‚ùå Erreur lecture phrase ${currentIndex}`, 'error');
                            reject(new Error('Erreur lecture audio'));
                        };
                    });

                    if (pipelineActive && currentIndex < sentences.length - 1) {
                        await new Promise(resolve => setTimeout(resolve, 50));
                    }

                } catch (error) {
                    log(`Erreur lecture phrase ${currentIndex}: ${error.message}`, 'error');
                }

                currentIndex++;
            }

            log('üéµ Pipeline audio termin√©', 'info');
        }

        async function speakTextWithPipeline(text, buttonElement = null) {
            if (isProcessingTTS) {
                log('TTS pipeline d√©j√† en cours', 'warning');
                return;
            }

            try {
                isProcessingTTS = true;
                isPlaying = true;
                pipelineActive = true;

                if (buttonElement) {
                    buttonElement.textContent = '‚è∏Ô∏è';
                    buttonElement.disabled = true;
                }

                const sentences = splitIntoSentences(text);
                if (sentences.length === 0) {
                    throw new Error('Aucune phrase d√©tect√©e');
                }

                readyAudioBuffer.clear();
                generationWorkers = [];

                log(`üöÄ D√©marrage pipeline TTS pour ${sentences.length} phrases`, 'info');

                const firstAudioUrl = await generateTTSForSentence(sentences[0], voiceSelect.value, 0);
                readyAudioBuffer.set(0, {
                    audioUrl: firstAudioUrl,
                    sentence: sentences[0],
                    index: 0
                });

                if (sentences.length > 1) {
                    const worker = pipelineWorker(sentences, voiceSelect.value, 1);
                    generationWorkers.push(worker);
                }

                await playAudioPipeline(sentences, voiceSelect.value);
                await cleanupPipeline();

                if (buttonElement) {
                    buttonElement.textContent = 'üîä';
                    buttonElement.disabled = false;
                }

                log('üéâ Pipeline TTS termin√© avec succ√®s', 'success');

            } catch (error) {
                console.error('Erreur TTS pipeline:', error);
                log(`Erreur pipeline: ${error.message}`, 'error');
            await cleanupPipeline();
            
            if (buttonElement) {
                buttonElement.textContent = 'üîä';
                buttonElement.disabled = false;
            }
        }
    }
    async function cleanupPipeline() {
    pipelineActive = false;
    isProcessingTTS = false;
    isPlaying = false;
    
    // Nettoyer le nouveau pipeline
    cleanupPhrasePipeline();

    for (const [key, data] of readyAudioBuffer) {
        URL.revokeObjectURL(data.audioUrl);
    }
    readyAudioBuffer.clear();

    await Promise.allSettled(generationWorkers);
    generationWorkers = [];

    if (!myVad) updateStatus("Arr√™t√©", "üò¥", "");
    else updateStatus("En √©coute...", "üëÇ", "listening");
}
    async function speakText(text, buttonElement = null) {
        return speakTextWithPipeline(text, buttonElement);
    }
    window.speakText = speakText;

    function showInterruptIndicator() { 
        if (interruptIndicator) {
            interruptIndicator.classList.add('opacity-100'); 
            setTimeout(() => interruptIndicator.classList.remove('opacity-100'), 1500); 
        }
    }

    function interruptTTS() { 
        if ((isProcessingTTS || isPlaying) && allowInterruptCheckbox?.checked) { 
            log('üõë INTERRUPTION TTS pipeline d√©tect√©e!', 'warning'); 
            
            pipelineActive = false;
            
            if (ttsAudio) {
                ttsAudio.pause(); 
                ttsAudio.currentTime = 0; 
            }
            
            cleanupPipeline();
            
            updateStatus("Interruption!", "‚ö°Ô∏è", "interrupted"); 
            showInterruptIndicator(); 
            addInterruptionMessage(); 
            setTimeout(() => updateStatus("En √©coute...", "üëÇ", "listening"), 800); 
            return true; 
        } 
        return false; 
    }

    // =============================================
    // üîß TRANSCRIPTION
    // =============================================
// üéØ NOUVELLE FONCTION TOGGLE
async function toggleRecording() {
    if (recordingMode === 'continu') {
        // Mode continu (comme avant)
        if (isRecording) {
            await stopRecording();
        } else {
            await startRecording();
        }
    } else {
        // Mode Push-to-Talk
        await handlePushToTalk();
    }
}
// üÜï NOUVEAU : Gestion du Push-to-Talk
async function handlePushToTalk() {
    if (isPlaying || isProcessingTTS) {
        // Si l'IA parle, couper l'audio
        log('üõë Interruption TTS par Push-to-Talk', 'warning');
        interruptTTS();
        return;
    }
    
    if (isPushToTalkRecording) {
        // Arr√™ter l'enregistrement et envoyer
        await stopPushToTalkRecording();
    } else {
        // D√©marrer l'enregistrement
        await startPushToTalkRecording();
    }
}

async function startPushToTalkRecording() {
    const groqKey = groqApiKeyInput?.value?.trim();
    const openaiKey = openaiApiKeyInput?.value?.trim();

    if (!groqKey || !openaiKey) { 
        alert('Veuillez entrer vos cl√©s API !'); 
        return; 
    }

    try {
        updateStatus("üé§ Parlez maintenant...", "üî¥", "recording");
        
        // Cr√©er un VAD pour le Push-to-Talk
        myVad = await vad.MicVAD.new({
            positiveSpeechThreshold: 0.6,
            minSpeechFrames: 3,
            preSpeechPadFrames: 1,
            redemptionFrames: 5,
            onSpeechStart: () => { 
                log("üé§ Parole d√©tect√©e en Push-to-Talk", 'info'); 
            },
            onSpeechEnd: (audio) => { 
                // Accumuler l'audio au lieu de le traiter imm√©diatement
                pushToTalkAudio.push(...audio);
                log(`Audio accumul√©: ${pushToTalkAudio.length} √©chantillons`, 'info');
            },
        });
        
        myVad.start();
        pushToTalkAudio = [];
        isPushToTalkRecording = true;
        updateButtonState();
        
        log("üé§ Push-to-Talk d√©marr√© - cliquez √† nouveau pour envoyer", 'success');
        
    } catch (error) {
        log(`Erreur Push-to-Talk: ${error.message}`, 'error');
        isPushToTalkRecording = false;
        updateButtonState();
    }
}

async function stopPushToTalkRecording() {
    if (myVad) {
        myVad.pause();
        myVad = null;
    }
    
    isPushToTalkRecording = false;
    updateButtonState();
    
    if (pushToTalkAudio.length > 1000) { // Au moins 1000 √©chantillons
        updateStatus("üîÑ Traitement de votre message...", "‚ö°", "processing");
        
        // Traiter tout l'audio accumul√©
        const fullAudio = new Float32Array(pushToTalkAudio);
        await transcribeAudioUltraFast(fullAudio);
        
        log(`‚úÖ Push-to-Talk envoy√©: ${fullAudio.length} √©chantillons`, 'success');
    } else {
        updateStatus("‚ùå Message trop court", "‚ö†Ô∏è", "");
      // REMPLACE PAR √ßa (imm√©diat) :
updateStatus("En √©coute...", "üëÇ", "listening");
        log("Message Push-to-Talk trop court, ignor√©", 'warning');
    }
    
    pushToTalkAudio = [];
}

// üÜï NOUVEAU : Gestion des modes
function switchToMode(mode) {
    recordingMode = mode;
    
    // Arr√™ter tout ce qui est en cours
    if (isRecording || isPushToTalkRecording) {
        if (recordingMode === 'continu') {
            stopRecording();
        } else {
            stopPushToTalkRecording();
        }
    }
    
    // Mettre √† jour l'interface
    updateModeButtons();
    updateButtonState();
    
    log(`Mode chang√© vers: ${mode}`, 'info');
}

function updateModeButtons() {
    if (recordingMode === 'continu') {
        modeContinu.className = "flex-1 py-2 px-3 rounded-lg text-sm font-medium transition-all bg-green-500/80 text-white";
        modePushToTalk.className = "flex-1 py-2 px-3 rounded-lg text-sm font-medium transition-all bg-white/20 text-white/70 hover:bg-white/30";
    } else {
        modeContinu.className = "flex-1 py-2 px-3 rounded-lg text-sm font-medium transition-all bg-white/20 text-white/70 hover:bg-white/30";
        modePushToTalk.className = "flex-1 py-2 px-3 rounded-lg text-sm font-medium transition-all bg-blue-500/80 text-white";
    }
}
async function startRecording() {
    const groqKey = groqApiKeyInput?.value?.trim();
    const openaiKey = openaiApiKeyInput?.value?.trim();

    if (!groqKey || !openaiKey) { 
        alert('Veuillez entrer vos cl√©s API Groq (STT) et OpenAI (GPT-4o Mini) !'); 
        return; 
    }

    // D√©sactiver le bouton pendant l'initialisation
    toggleButton.disabled = true;
    updateStatus("Initialisation...", "‚ö°", "processing");
    
    try {
        // Pr√©-g√©n√©rer les r√©flexions d'abord
        await preGenerateSimpleReflections();
        
        myVad = await vad.MicVAD.new({
            positiveSpeechThreshold: 0.8,
            minSpeechFrames: 6,
            preSpeechPadFrames: 1,
            redemptionFrames: 10,
            onSpeechStart: () => { 
                log("Parole d√©tect√©e", 'success'); 
                if (!interruptTTS()) updateStatus("Vous parlez!", "üó£Ô∏è", "speaking"); 
            },
            onSpeechEnd: async (audio) => { 
    log(`Fin de parole`, 'info');
    
    if (!isProcessingTTS && audio.length > 600) {
        // üî• R√âFLEXION IMM√âDIATE (0ms de d√©lai)
        const reflectionPromise = playRandomReflectionAsync(); // Pas d'await !
        
        // üî• PROCESSING IMM√âDIAT EN PARALL√àLE
        const processingPromise = processUserMessageParallel(audio);
        
        // üî• LANCER TOUT SANS ATTENDRE
        handleParallelPipeline(reflectionPromise, processingPromise);
    } else if (myVad) {
        updateStatus("En √©coute...", "üëÇ", "listening");
    }
}
        });
        
        myVad.start();
        
        // Changer l'√©tat du bouton
        isRecording = true;
        toggleButton.disabled = false;
        updateButtonState();
        
        updateStatus("En √©coute...", "üëÇ", "listening");
        log("üé§ Enregistrement d√©marr√©!", 'success');
        
    } catch (error) { 
        console.error("Erreur VAD:", error); 
        log(`Erreur VAD: ${error.message}`, 'error'); 
        updateStatus("Erreur", "‚ùå", ""); 
        toggleButton.disabled = false;
        isRecording = false;
        updateButtonState();
    }
}

async function stopRecording() {
    if (myVad) { 
        myVad.pause(); 
        myVad = null; 
        log("VAD arr√™t√©", 'info'); 
    }
    
    await cleanupPipeline();
    
    // Nettoyer les r√©flexions
    for (const [key, url] of reflectionAudios) {
        URL.revokeObjectURL(url);
    }
    reflectionAudios.clear();
    isPlayingReflection = false;
    
    // Changer l'√©tat du bouton
    isRecording = false;
    updateButtonState();
    
    updateStatus("Arr√™t√©", "üò¥", "");
    log("üõë Enregistrement arr√™t√©", 'info');
}


// =============================================
// üéµ NOUVELLE FONCTION DE PAROLE EN STREAMING
// =============================================
function addToConversation(type, content) {
    const conversationDiv = document.getElementById('conversation'); // Cible le bon div

    if (!conversationDiv) return;
    
    // Ne supprimer le message d'accueil qu'une seule fois
    if (conversationDiv.querySelector('.italic')) {
        conversationDiv.innerHTML = '';
    }
    
    const div = document.createElement('div');
    const baseClasses = 'mb-3 px-4 py-3 rounded-2xl max-w-[85%] break-words';
    let specificClasses = '';
    let header = '';

    if (type === 'user') {
        specificClasses = 'bg-blue-500/80 text-white ml-auto rounded-br-md';
        header = `<div class="text-xs opacity-80 mb-1">Vous ‚Ä¢ ${new Date().toLocaleTimeString()}</div>`;
        conversationHistory.push({ role: 'user', content: content });
    } else { // 'assistant'
        specificClasses = 'bg-white/90 text-gray-800 mr-auto rounded-bl-md';
        // On passe le contenu √† speakTextWithStreaming au lieu de l'ancien speakText
        header = `<div class="text-xs opacity-70 mb-1 flex justify-between items-center"><span>Assistant ‚Ä¢ ${new Date().toLocaleTimeString()}</span><button onclick="speakTextWithStreaming('${content.replace(/'/g, "\\'")}', this)" class="bg-transparent border-none cursor-pointer text-lg hover:scale-110 transition-transform">üîä</button></div>`;
        conversationHistory.push({ role: 'assistant', content: content });
        
        // ‚ú® MODIFICATION CI-DESSOUS ‚ú®
        // D√©clencher la parole automatiquement si la case est coch√©e
        if (autoSpeakCheckbox?.checked && !isProcessingTTS) {
            // Un petit d√©lai pour s'assurer que l'interface est √† jour avant de parler
            setTimeout(() => speakTextWithPipeline(content), 100);
        }
    }

    div.className = `${baseClasses} ${specificClasses}`;
    div.innerHTML = `${header}<div>${content}</div>`;
    conversationDiv.appendChild(div);
    conversationDiv.scrollTop = conversationDiv.scrollHeight;

    // Limiter l'historique
    if (conversationHistory.length > 30) {
        conversationHistory.splice(0, conversationHistory.length - 30);
    }
}


async function speakTextWithStreaming(text, buttonElement = null) {
    // V√©rifie si les cl√©s sont l√† et initialise le client si ce n'est pas d√©j√† fait
    const cartesiaApiKey = "sk_car_j18nkdGqMkXxdvYwPJVJiJ"; // ‚ö†Ô∏è Remplace par ta vraie cl√© API Cartesia
    if (!cartesiaApiKey) {
        log("Cl√© API Cartesia manquante.", "error");
        return;
    }
    if (!cartesiaClient) {
        cartesiaClient = new cartesia.CartesiaClient({ apiKey: cartesiaApiKey });
    }

    if (isProcessingTTS) {
        log('TTS (streaming) d√©j√† en cours', 'warning');
        return;
    }

    try {
        isProcessingTTS = true;
        isPlaying = true;

        if (buttonElement) buttonElement.disabled = true;

        log('‚ö° D√©marrage du streaming TTS via WebSocket...', 'info');
        updateStatus("IA parle...", "üîä", "tts-playing");

        // 1. Cr√©e un lecteur audio pour le navigateur
        const player = new cartesia.WebPlayer();

        // 2. Ouvre la connexion WebSocket pour le TTS
        const ttsStream = cartesiaClient.tts.websocket({
            container: "raw",
            encoding: "pcm_f32le",
            sample_rate: 24000,
        });

        // 3. Envoie la demande de g√©n√©ration avec le texte complet
        const { stream } = await ttsStream.send({
            model_id: "sonic-2",
            transcript: text,
            voice: {
                mode: "id",
                id: "0b1380da-611b-4d00-83f4-8a969a53e4e0", // ID de la voix "Antoine"
            },
            // On peut ajouter un context_id pour garder la prosodie sur plusieurs tours
        });

        // 4. Joue le flux audio au fur et √† mesure qu'il arrive
        // Le `player.play` g√®re tout pour nous !
        await player.play(stream);

        // La lecture est termin√©e
        log('‚úÖ Streaming TTS termin√©.', 'success');
        
    } catch (error) {
        console.error('Erreur TTS streaming:', error);
        log(`Erreur streaming TTS: ${error.message}`, 'error');
    } finally {
        // Nettoyage apr√®s la parole
        isProcessingTTS = false;
        isPlaying = false;

        if (buttonElement) {
            buttonElement.textContent = 'üîä';
            buttonElement.disabled = false;
        }

        if (myVad) { // Si le micro est toujours actif
            updateStatus("En √©coute...", "üëÇ", "listening");
        } else {
            updateStatus("Arr√™t√©", "üò¥", "");
        }
    }
}


function updateButtonState() {
    if (!toggleButton || !toggleIcon || !toggleText) return;
    
    if (recordingMode === 'push-to-talk') {
        // Mode Push-to-Talk
        if (isPushToTalkRecording) {
            // En cours d'enregistrement
            toggleButton.className = "flex-1 py-4 rounded-2xl font-semibold text-base transition-all duration-300 backdrop-blur-xl bg-red-500/80 hover:bg-red-600/80 text-white animate-pulse flex items-center justify-center gap-2";
            toggleIcon.setAttribute('data-lucide', 'square');
            toggleText.textContent = 'Envoyer';
        } else {
            // Pr√™t √† enregistrer
            toggleButton.className = "flex-1 py-4 rounded-2xl font-semibold text-base transition-all duration-300 backdrop-blur-xl bg-blue-500/80 hover:bg-blue-600/80 text-white flex items-center justify-center gap-2";
            toggleIcon.setAttribute('data-lucide', 'mic');
            toggleText.textContent = 'Maintenir & Parler';
        }
    } else {
        // Mode continu (comme avant)
        if (isRecording) {
            // √âtat "Arr√™ter" (bouton rouge)
            toggleButton.className = "flex-1 py-4 rounded-2xl font-semibold text-base transition-all duration-300 backdrop-blur-xl bg-red-500/80 hover:bg-red-600/80 text-white disabled:opacity-50 disabled:cursor-not-allowed flex items-center justify-center gap-2";
            toggleIcon.setAttribute('data-lucide', 'mic-off');
            toggleText.textContent = 'Arr√™ter';
            log('üî¥ Bouton en mode STOP', 'info');
        } else {
            // √âtat "D√©marrer" (bouton vert)
            toggleButton.className = "flex-1 py-4 rounded-2xl font-semibold text-base transition-all duration-300 backdrop-blur-xl bg-green-500/80 hover:bg-green-600/80 text-white disabled:opacity-50 disabled:cursor-not-allowed flex items-center justify-center gap-2";
            toggleIcon.setAttribute('data-lucide', 'mic');
            toggleText.textContent = 'D√©marrer';
            log('üü¢ Bouton en mode START', 'info');
        }
    }
    
    // IMPORTANT : Mettre √† jour les ic√¥nes Lucide
    lucide.createIcons();
}
async function transcribeAudioUltraFast(audioData) {
        const startTime = performance.now();
        const groqKey = groqApiKeyInput?.value?.trim();
        
        if (!groqKey) { 
            alert('Veuillez entrer votre cl√© API Groq'); 
            return; 
        }
        
        try {
            const qualityResult = analyzeAudioQuality(audioData);
            updateQuality(qualityResult.quality);
            if (!qualityResult.pass) { 
                updateStatus("En √©coute...", "üëÇ", "listening"); 
                return; 
            }
            
            const audioHash = hashAudio(audioData);
            const cachedResult = getCachedTranscription(audioHash);
            if (cachedResult) {
                log('Transcription du cache utilis√©e', 'success');
                updateLatency(Math.round(performance.now() - startTime));
                await processUserMessageOptimized(cachedResult);
                return;
            }
            
            updateStatus("Transcription...", "‚ö°", "processing");
            const compressedAudio = compressAudio(audioData);
            const wavBuffer = encodeWAV(compressedAudio);
            const formData = new FormData();
            formData.append('file', new Blob([wavBuffer], { type: 'audio/wav' }), 'audio.wav');
            formData.append('model', 'whisper-large-v3');
            formData.append('response_format', 'json');
            formData.append('language', 'fr');
            
            const controller = new AbortController();
            setTimeout(() => controller.abort(), 10000);
            
            const transResponse = await fetch('https://api.groq.com/openai/v1/audio/transcriptions', {
                method: 'POST',
                headers: { 'Authorization': `Bearer ${groqKey}` },
                body: formData,
                signal: controller.signal
            });
            
            if (!transResponse.ok) throw new Error(`Transcription √©chou√©e: ${transResponse.status}`);
            const transResult = await transResponse.json();
            let userText = transResult.text?.trim();
//supe rfilytre anti parasite 
            const parasiteFilters = [
        /sous[\-\s]*titrage/i,
        /soci√©t√©[\-\s]*radio[\-\s]*canada/i,
        /radio[\-\s]*canada/i,
        /src[\-\s]*subtitle/i
    ];

    for (const filter of parasiteFilters) {
        if (filter.test(userText)) {
            log('Texte parasite d√©tect√© et ignor√©: ' + userText, 'warning');
            updateStatus("En √©coute...", "üëÇ", "listening");
            return;
        }
    }
    
    if (!userText || userText.length < 3) { 
        log('Aucune parole d√©tect√©e', 'warning'); 
        updateStatus("En √©coute...", "üëÇ", "listening"); 
        return; 
    }

            if (!userText) { 
                log('Aucune parole d√©tect√©e', 'warning'); 
                updateStatus("En √©coute...", "üëÇ", "listening"); 
                return; 
            }
            
            setCachedTranscription(audioHash, userText);
            updateLatency(Math.round(performance.now() - startTime));
            await processUserMessageOptimized(userText);
        } catch (error) {
            console.error('Erreur transcription:', error);
            log(`Erreur transcription: ${error.message}`, 'error');
            updateStatus("En √©coute...", "üëÇ", "listening");
        }
    }

    // =============================================
    // üîß LLM OPENAI
    // =============================================
    // =============================================
// üîß LLM GROQ ULTRA-RAPIDE AVEC STREAMING
async function getLLMResponseStreaming(userMessage) {
    const openaiKey = openaiApiKeyInput?.value?.trim();
    
    if (!openaiKey) {
        alert('Veuillez entrer votre cl√© API OpenAI pour utiliser GPT-4o Mini');
        return null;
    }
    
    try {
        log('üöÄ G√©n√©ration streaming OpenAI GPT-4o Mini...', 'info');
        updateStatus("IA r√©fl√©chit...", "ü§î", "processing");
        
        // NOUVELLE PARTIE - Construction des messages avec contexte de r√©flexion
        const messages = [
            { 
                role: "system", 
                content: `Tu es un assistant vocal conversationnel naturel. R√©ponds en fran√ßais parl√©, comme dans une conversation normale entre humains.

R√àGLES IMPORTANTES :
- COMMENCE TOUJOURS par une phrase tr√®s courte (3-5 mots maximum) etcontinue avec des phrases tres courtes.
- N'utilise JAMAIS d'√©motic√¥nes, d'ast√©risques, ou de symboles (‚ù§Ô∏è, üî•, *, etc.)
- Parle comme si tu √©tais une personne r√©elle au t√©l√©phone
- Utilise un langage oral naturel avec des expressions comme "alors", "bon", "eh bien"
- Reste concis et direct, maximum 2-3 phrases
- N'√©cris pas d'actions entre ast√©risques comme "*sourit*" ou "*r√©fl√©chit*"
- √âvite les listes √† puces ou les formats structur√©s
- R√©ponds simplement et naturellement comme dans une vraie conversation`
            }, 
            ...conversationHistory.slice(-30),
            { role: 'user', content: userMessage }
        ];
        
        // NOUVELLE PARTIE - Ajouter le contexte de la r√©flexion si elle existe
        if (lastPlayedReflection && lastPlayedReflection.trim() !== "") {
            messages.push({
                role: "system", 
                content: `INFO CONTEXTE VOCAL: Tu viens de dire cette phrase √† l'utilisateur: "${lastPlayedReflection}"

R√àGLE ABSOLUE: Ne r√©p√®te JAMAIS cette phrase dans ta r√©ponse. Commence directement par le contenu de ta r√©ponse sans r√©p√©ter "${lastPlayedReflection}".`
            });
            log(`üéØ Contexte ajout√©: r√©flexion "${lastPlayedReflection}" d√©j√† jou√©e`, 'info');
        }
        
        // üî• STREAMING REQUEST avec OpenAI GPT-4o Mini
        const response = await fetch('https://api.openai.com/v1/chat/completions', {
            method: 'POST',
            headers: { 
                'Authorization': `Bearer ${openaiKey}`, 
                'Content-Type': 'application/json' 
            },
            body: JSON.stringify({ 
                model: 'gpt-4o-mini',
                messages: messages, 
                temperature: 0.3,
                max_tokens: 150,
                top_p: 0.9,
                stream: true
            })
        });
        
        if (!response.ok) {
            const errorData = await response.json().catch(() => ({}));
            throw new Error(`HTTP ${response.status}: ${errorData.error?.message || response.statusText}`);
        }
        
        // üî• LECTURE STREAMING ET ACCUMULATION
        let fullResponse = '';
        const reader = response.body.getReader();
        const decoder = new TextDecoder();
        
        while (true) {
            const { done, value } = await reader.read();
            if (done) break;
            
            const chunk = decoder.decode(value);
            const lines = chunk.split('\n');
            
            for (const line of lines) {
                if (line.startsWith('data: ')) {
                    const data = line.slice(6);
                    if (data === '[DONE]') break;
                    
                    try {
                        const parsed = JSON.parse(data);
                        const content = parsed.choices[0]?.delta?.content || '';
                        
                        if (content) {
                            fullResponse += content;
                        }
                    } catch (parseError) {
                        // Ignorer les erreurs de parsing JSON
                    }
                }
            }
        }
        
        const finalResponse = fullResponse.trim() || 'D√©sol√©, je n\'ai pas pu r√©pondre.';
        
        // NOUVELLE PARTIE - Nettoyer la r√©ponse si elle r√©p√®te la r√©flexion
        let cleanedResponse = finalResponse;
        if (lastPlayedReflection && lastPlayedReflection.trim() !== "") {
            // V√©rifier si la r√©ponse commence par la r√©flexion
            const reflectionLower = lastPlayedReflection.toLowerCase().trim();
            const responseLower = finalResponse.toLowerCase().trim();
            
            if (responseLower.startsWith(reflectionLower)) {
                cleanedResponse = finalResponse.substring(lastPlayedReflection.length).trim();
                // Enlever les virgules ou points qui pourraient rester
                cleanedResponse = cleanedResponse.replace(/^[,.\s]+/, '');
                log(`üßπ R√©ponse nettoy√©e: "${lastPlayedReflection}" retir√©`, 'info');
            }
        }
        
        // R√©initialiser la r√©flexion pour la prochaine fois
        lastPlayedReflection = "";
        
//         // üéµ JOUER LE TTS COMPLET UNE SEULE FOIS
//         if (autoSpeakCheckbox?.checked && cleanedResponse) {
//             log(`üéµ Lancement TTS pour r√©ponse nettoy√©e: "${cleanedResponse.substring(0, 50)}..."`, 'info');
//             // Attendre un peu pour √©viter les conflits
//            // REMPLACE PAR √ßa (sans d√©lai) :
// speakTextWithPipeline(cleanedResponse);
//         }
        
        log('‚úÖ Streaming OpenAI GPT-4o Mini termin√©', 'success');
        return cleanedResponse; // Retourner la r√©ponse nettoy√©e
        
    } catch (error) {
        console.error('Erreur OpenAI GPT-4o Mini:', error);
        log(`Erreur OpenAI: ${error.message}`, 'error');
        // R√©initialiser la r√©flexion en cas d'erreur aussi
        lastPlayedReflection = "";
        return `Erreur OpenAI: ${error.message}`;
    }
}



async function processUserMessageOptimized(userText) {
    if (!userText) return;

    if (userText.toLowerCase().includes('fin de discussion')) {
        log('Commande "Fin de discussion" d√©tect√©e.', 'info');
        stop();
        addSystemMessage('Session termin√©e par commande vocale.');
        return;
    }

    log(`Message re√ßu: "${userText}"`, 'info');
    addToConversation('user', userText);

    // üöÄ UTILISER LA NOUVELLE FONCTION STREAMING
    const aiResponse = await getLLMResponseStreaming(userText);
    if (aiResponse) {
        addToConversation('assistant', aiResponse);
    }
    
    if (myVad) {
        updateStatus("En √©coute...", "üëÇ", "listening");
    }
}
    // =============================================
    // üîß CONVERSATION
    // =============================================

    function addSystemMessage(content) { 
        if (!conversation) return;
        if (conversation.querySelector('.italic')) conversation.innerHTML = ''; 
        const div = document.createElement('div'); 
        div.className = 'my-4 text-center text-xs text-white/50 italic'; 
        div.textContent = `--- ${content} ---`; 
        conversation.appendChild(div); 
        conversation.scrollTop = conversation.scrollHeight; 
    }

    function addInterruptionMessage() { 
        if (!conversation) return;
        const div = document.createElement('div'); 
        div.className = 'mb-3 p-3 rounded-lg border bg-red-500/10 border-red-500/30 text-white/70 italic'; 
        div.innerHTML = `<div class="text-xs opacity-80 mb-1">Syst√®me ‚Ä¢ ${new Date().toLocaleTimeString()}</div><div>‚ö°Ô∏è R√©ponse interrompue</div>`; 
        conversation.appendChild(div); 
        conversation.scrollTop = conversation.scrollHeight; 
    }



    async function startRecording() {
    const groqKey = groqApiKeyInput?.value?.trim();
    const openaiKey = openaiApiKeyInput?.value?.trim();

    if (!groqKey || !openaiKey) {
        alert('Veuillez entrer vos cl√©s API Groq (STT) et OpenAI (LLM) !');
        return;
    }

    toggleButton.disabled = true;
    updateStatus("Initialisation...", "‚ö°", "processing");

    try {
        // Tu peux garder la pr√©-g√©n√©ration des r√©flexions, c'est une bonne id√©e.
        //await preGenerateSimpleReflections();

        myVad = await vad.MicVAD.new({
            positiveSpeechThreshold: 0.8,
            minSpeechFrames: 6,
            preSpeechPadFrames: 1,
            redemptionFrames: 10,
            onSpeechStart: () => {
                log("Parole d√©tect√©e", 'success');
                if (!interruptTTS()) updateStatus("Vous parlez!", "üó£Ô∏è", "speaking");
            },
            onSpeechEnd: async (audio) => {
                log(`Fin de parole d√©tect√©e.`, 'info');

                if (isProcessingTTS || audio.length < 800) {
                    if (myVad) updateStatus("En √©coute...", "üëÇ", "listening");
                    return; // Ignore les sons trop courts ou si l'IA parle d√©j√†
                }

                // ‚ú® NOUVELLE LOGIQUE SIMPLIFI√âE ‚ú®
                
                // 1. Joue une r√©flexion en parall√®le (fire-and-forget)
                // playRandomReflectionAsync(); // Optionnel, tu peux d√©commenter si tu veux

                // 2. Transcris l'audio de l'utilisateur
                const userText = await transcribeAudioFromBuffer(audio);
                if (!userText) {
                    if (myVad) updateStatus("En √©coute...", "üëÇ", "listening");
                    return;
                }
                
                // 3. Ajoute le message de l'utilisateur √† la conversation
                addToConversation('user', userText);

                // 4. Obtient la r√©ponse de l'IA
                const aiResponse = await getLLMResponseStreaming(userText);
                if (!aiResponse) {
                    if (myVad) updateStatus("En √©coute...", "üëÇ", "listening");
                    return;
                }

                // 5. Ajoute la r√©ponse de l'IA √† la conversation
                // Note: La fonction addToConversation va automatiquement lancer la parole en streaming !
                addToConversation('assistant', aiResponse);
            },
        });

        myVad.start();
        isRecording = true;
        toggleButton.disabled = false;
        updateButtonState();
        updateStatus("En √©coute...", "üëÇ", "listening");
        log("üé§ Enregistrement d√©marr√©!", 'success');

    } catch (error) {
        console.error("Erreur VAD:", error);
        log(`Erreur VAD: ${error.message}`, 'error');
        updateStatus("Erreur", "‚ùå", "");
        toggleButton.disabled = false;
        isRecording = false;
        updateButtonState();
    }
}


    async function processUserMessage(userText) {
        if (!userText) return;

        if (userText.toLowerCase().includes('fin de discussion')) {
            log('Commande "Fin de discussion" d√©tect√©e.', 'info');
            stop();
            addSystemMessage('Session termin√©e par commande vocale.');
            return;
        }

        log(`Message re√ßu: "${userText}"`, 'info');
        addToConversation('user', userText);

        const aiResponse = await getLLMResponse(userText);
        if (aiResponse) {
            addToConversation('assistant', aiResponse);
        }
        
        if (myVad) {
            updateStatus("En √©coute...", "üëÇ", "listening");
        }
    }

    // =============================================
    // üîß FONCTIONS PRINCIPALES
    // =============================================
    
// Ajoute cette fonction dans ton script
async function requestMicrophonePermission() {
    try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        stream.getTracks().forEach(track => track.stop());
        log('Permission microphone accord√©e', 'success');
        return true;
    } catch (error) {
        log('Permission microphone refus√©e', 'error');
        alert('Veuillez autoriser l\'acc√®s au microphone dans les param√®tres Safari');
        return false;
    }
}



async function start() {
    const hasPermission = await requestMicrophonePermission();
    if (!hasPermission) {
        startButton.disabled = false;
        return;
    }
    const groqKey = groqApiKeyInput?.value?.trim();
const openaiKey = openaiApiKeyInput?.value?.trim();

if (!groqKey || !openaiKey) { 
alert('Veuillez entrer vos cl√©s API Groq (pour STT) et OpenAI (pour GPT-4o Mini) !'); 
return; 
}

console.log('üîë Cl√©s utilis√©es:', { groq: groqKey ? 'OK (STT)' : 'KO', openai: openaiKey ? 'OK (GPT-4o Mini)' : 'KO' });

    startButton.disabled = true;
    updateStatus("Initialisation...", "‚ö°", "processing");
    
    try {
        // üí≠ PR√â-G√âN√âRER LES R√âFLEXIONS D'ABORD
        await preGenerateSimpleReflections();
        
        myVad = await vad.MicVAD.new({
            positiveSpeechThreshold: 0.6, // Baiss√© de 0.8 √† 0.6
    minSpeechFrames: 4, // Baiss√© de 6 √† 4
    preSpeechPadFrames: 0, // R√©duit de 1 √† 0
    redemptionFrames: 6, // Baiss√© de 10 √† 6
    // Ajouter ces nouveaux param√®tres
    frameSamples: 512, // Plus petit = plus r√©actif
            onSpeechStart: () => { 
                log("Parole d√©tect√©e", 'success'); 
                if (!interruptTTS()) updateStatus("Vous parlez!", "üó£Ô∏è", "speaking"); 
            },
            onSpeechEnd: (audio) => { 
                log(`Fin de parole`, 'info');
                
                // üí≠ JOUER R√âFLEXION IMM√âDIATEMENT !
                playRandomReflection();
                
                if (!isProcessingTTS && audio.length > 600) {
                    transcribeAudioUltraFast(audio);
                } else if (myVad) {
                    updateStatus("En √©coute...", "üëÇ", "listening");
                }
            },
        });
        
        myVad.start();
        updateStatus("En √©coute...", "üëÇ", "listening");
        stopButton.disabled = false;
        log("Syst√®me vocal avec r√©flexions activ√©!", 'success');
        
    } catch (error) { 
        console.error("Erreur VAD:", error); 
        log(`Erreur VAD: ${error.message}`, 'error'); 
        updateStatus("Erreur", "‚ùå", ""); 
        startButton.disabled = false; 
    }
}
function stop() {
    if (myVad) { 
        myVad.pause(); 
        myVad = null; 
        log("VAD arr√™t√©", 'info'); 
    }
    
    cleanupPipeline();
    
    // Nettoyer les r√©flexions
    for (const [key, url] of reflectionAudios) {
        URL.revokeObjectURL(url);
    }
    reflectionAudios.clear();
    isPlayingReflection = false;
    
    updateStatus("Arr√™t√©", "üò¥", "");
    startButton.disabled = false;
    stopButton.disabled = true;
    log("Pipeline TTS et r√©flexions arr√™t√©s", 'info');
}
    function clearConversation() {
        if (conversation) {
            conversation.innerHTML = `<div class="text-white/60 text-center italic mt-10">Appuyez sur d√©marrer ou envoyez un message...<br><small class="mt-2 block">‚ö° Mode Ultra-Rapide activ√©!</small></div>`;
            conversationHistory = [];
            log("Conversation effac√©e", 'info');
        }
    }
    
    async function handleTextInput() {
        const userText = textInput?.value?.trim();
        if (userText) {
            textInput.value = '';
            await processUserMessageOptimized(userText);
        }
    }
    
    // =============================================
    // üöÄ INITIALISATION PRINCIPALE
    // =============================================
    
    window.onload = () => {
// Ajoute √ßa dans ton window.onload

// üîß CORRECTION : D√©clarer les variables AVANT de les utiliser
const conversationToggle = document.getElementById('conversationToggle');
const conversationDrawer = document.getElementById('conversationDrawer');
const conversationDrawerClose = document.getElementById('conversationDrawerClose');

function toggleConversationDrawer() {
    if (conversationDrawer && conversationDrawer.classList.contains('-translate-x-full')) {
        // Ouvrir
        conversationDrawer.classList.remove('-translate-x-full');
        log('Drawer conversations ouvert', 'info');
    } else if (conversationDrawer) {
        // Fermer
        conversationDrawer.classList.add('-translate-x-full');
        log('Drawer conversations ferm√©', 'info');
    }
}

// Attacher les √©v√©nements APR√àS avoir d√©clar√© les variables
if (conversationToggle) {
    conversationToggle.onclick = toggleConversationDrawer;
}

if (conversationDrawerClose) {
    conversationDrawerClose.onclick = toggleConversationDrawer;
}

// üÜï NOUVEAU : Initialisation des modes
modeContinu = document.getElementById('modeContinu');
modePushToTalk = document.getElementById('modePushToTalk');

// √âv√©nements des boutons de mode
if (modeContinu) modeContinu.onclick = () => switchToMode('continu');
if (modePushToTalk) modePushToTalk.onclick = () => switchToMode('push-to-talk');

// Initialiser l'affichage des modes
updateModeButtons();

// Cacher la zone conversation principale
if (document.getElementById('conversation-area')) {
    document.getElementById('conversation-area').style.display = 'none';
}



function toggleConversationDrawer() {
    if (conversationDrawer.classList.contains('-translate-x-full')) {
        // Ouvrir
        conversationDrawer.classList.remove('-translate-x-full');
        log('Drawer conversations ouvert', 'info');
    } else {
        // Fermer
        conversationDrawer.classList.add('-translate-x-full');
        log('Drawer conversations ferm√©', 'info');
    }
}

// Attacher l'√©v√©nement
if (conversationToggle) {
    conversationToggle.onclick = toggleConversationDrawer;
}



// ===============================
// üé® DRAWER FUNCTIONALITY
// ===============================

const drawer = document.getElementById('drawer');
const drawerOverlay = document.getElementById('drawerOverlay');
const drawerToggle = document.getElementById('drawerToggle');
const drawerClose = document.getElementById('drawerClose');

// √âl√©ments du drawer
const drawerGroqKey = document.getElementById('drawerGroqKey');
const drawerOpenaiKey = document.getElementById('drawerOpenaiKey');
const drawerAutoSpeak = document.getElementById('drawerAutoSpeak');
const drawerAllowInterrupt = document.getElementById('drawerAllowInterrupt');
const drawerVoiceSelect = document.getElementById('drawerVoiceSelect');
const drawerDebug = document.getElementById('drawerDebug');

// Status indicators
const groqStatus = document.getElementById('groqStatus');
const openaiStatus = document.getElementById('openaiStatus');

function openDrawer() {
    drawer.classList.remove('translate-x-full');
    drawerOverlay.classList.remove('opacity-0', 'pointer-events-none');
    document.body.style.overflow = 'hidden';
    
    // Synchroniser les valeurs
    if (drawerGroqKey) drawerGroqKey.value = groqApiKeyInput?.value || '';
    if (drawerOpenaiKey) drawerOpenaiKey.value = openaiApiKeyInput?.value || '';
    if (drawerAutoSpeak) drawerAutoSpeak.checked = autoSpeakCheckbox?.checked || false;
    if (drawerAllowInterrupt) drawerAllowInterrupt.checked = allowInterruptCheckbox?.checked || false;
    if (drawerVoiceSelect) drawerVoiceSelect.value = voiceSelect?.value || 'fr-FR-DeniseNeural';
    
    log('Drawer ouvert', 'info');
}

function closeDrawer() {
    drawer.classList.add('translate-x-full');
    drawerOverlay.classList.add('opacity-0', 'pointer-events-none');
    document.body.style.overflow = '';
    log('Drawer ferm√©', 'info');
}

function updateStatusIndicators() {
    if (groqStatus) {
        groqStatus.className = groqApiKeyInput?.value?.trim() ? 
            'w-2 h-2 rounded-full bg-green-400' : 
            'w-2 h-2 rounded-full bg-red-400';
    }
    if (openaiStatus) {
        openaiStatus.className = openaiApiKeyInput?.value?.trim() ? 
            'w-2 h-2 rounded-full bg-green-400' : 
            'w-2 h-2 rounded-full bg-red-400';
    }
}

// Event listeners pour le drawer
if (drawerToggle) drawerToggle.onclick = openDrawer;
if (drawerClose) drawerClose.onclick = closeDrawer;
if (drawerOverlay) drawerOverlay.onclick = closeDrawer;

// Synchronisation bidirectionnelle
if (drawerGroqKey) {
    drawerGroqKey.oninput = function() {
        if (groqApiKeyInput) groqApiKeyInput.value = this.value;
        localStorage.setItem('groq_api_key', this.value);
        updateStatusIndicators();
        console.log('üî• GROQ depuis drawer:', this.value);
    };
}

if (drawerOpenaiKey) {
    drawerOpenaiKey.oninput = function() {
        if (openaiApiKeyInput) openaiApiKeyInput.value = this.value;
        localStorage.setItem('openai_api_key', this.value);
        updateStatusIndicators();
        console.log('üî• OPENAI depuis drawer:', this.value);
    };
}

// Actions rapides
document.getElementById('drawerClearCache')?.addEventListener('click', () => {
    transcriptionCache.clear();
    ttsCache.clear();
    log('Cache vid√© depuis drawer', 'success');
});

document.getElementById('drawerExportConversation')?.addEventListener('click', () => {
    const data = JSON.stringify(conversationHistory, null, 2);
    const blob = new Blob([data], { type: 'application/json' });
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = `conversation_${new Date().toISOString().slice(0,10)}.json`;
    a.click();
    URL.revokeObjectURL(url);
    log('Conversation export√©e', 'success');
});

document.getElementById('drawerResetSettings')?.addEventListener('click', () => {
    if (confirm('R√©initialiser tous les param√®tres ?')) {
        localStorage.clear();
        location.reload();
    }
});

// Fermer avec Escape
document.addEventListener('keydown', (e) => {
    if (e.key === 'Escape') {
        closeDrawer();
    }
});

// Mise √† jour initiale des indicateurs
updateStatusIndicators();

// Am√©liorer la fonction log pour aussi logger dans le drawer
const originalLog = log;
log = function(message, type = 'info') {
    originalLog(message, type);
    
    // Ajouter aussi dans le drawer debug
    if (drawerDebug) {
        const timestamp = new Date().toLocaleTimeString();
        const emoji = type === 'success' ? '‚úÖ' : type === 'error' ? '‚ùå' : type === 'warning' ? '‚ö†Ô∏è' : '‚ö°';
        const logMessage = `${timestamp} ${emoji} ${message}`;
        
        const newLine = document.createElement('div');
        newLine.textContent = logMessage;
        newLine.className = type === 'error' ? 'text-red-300' : type === 'success' ? 'text-green-300' : type === 'warning' ? 'text-yellow-300' : '';
        drawerDebug.insertBefore(newLine, drawerDebug.firstChild);
        
        while (drawerDebug.children.length > 20) {
            drawerDebug.removeChild(drawerDebug.lastChild);
        }
    }
};

console.log('‚ú® Drawer initialis√© avec succ√®s !');



        console.log('üöÄ Initialisation du syst√®me complet...');
        
        // ‚úÖ R√âCUP√âRER TOUS LES √âL√âMENTS
        groqApiKeyInput = document.getElementById('groqApiKey');
        openaiApiKeyInput = document.getElementById('openaiApiKey');
       // REMPLACER CES LIGNES :
// startButton = document.getElementById('startButton');
// stopButton = document.getElementById('stopButton');

// PAR CES NOUVELLES LIGNES :
toggleButton = document.getElementById('toggleButton');
toggleIcon = document.getElementById('toggleIcon');
toggleText = document.getElementById('toggleText');
        clearButton = document.getElementById('clearButton');
        voiceCircle = document.getElementById('voice-circle');
        statusText = document.getElementById('status-text');
        debug = document.getElementById('debug');
        conversation = document.getElementById('conversation');
        autoSpeakCheckbox = document.getElementById('autoSpeak');
        allowInterruptCheckbox = document.getElementById('allowInterrupt');
        voiceSelect = document.getElementById('voiceSelect');
        ttsAudio = document.getElementById('ttsAudio');
        interruptIndicator = document.getElementById('interrupt-indicator');
        processingIndicator = document.getElementById('processing-indicator');
        cacheHitsElement = document.getElementById('cache-hits');
        latencyElement = document.getElementById('latency');
        qualityElement = document.getElementById('quality');
        textInput = document.getElementById('text-input');
        sendButton = document.getElementById('send-button');
        
        // ‚úÖ V√âRIFIER QUE TOUT EXISTE
        console.log('Tests √©l√©ments:');
        console.log('- groqApiKeyInput:', groqApiKeyInput ? 'OK' : 'ERREUR');
        console.log('- openaiApiKeyInput:', openaiApiKeyInput ? 'OK' : 'ERREUR');
        console.log('- startButton:', startButton ? 'OK' : 'ERREUR');
        console.log('- voiceCircle:', voiceCircle ? 'OK' : 'ERREUR');
        console.log('- conversation:', conversation ? 'OK' : 'ERREUR');
        
        // ‚úÖ CHARGER LES VALEURS SAUVEGARD√âES
        if (groqApiKeyInput) groqApiKeyInput.value = localStorage.getItem('groq_api_key') || '';
        if (openaiApiKeyInput) openaiApiKeyInput.value = localStorage.getItem('openai_api_key') || '';
        if (autoSpeakCheckbox) autoSpeakCheckbox.checked = localStorage.getItem('auto_speak') !== 'false';
        if (allowInterruptCheckbox) allowInterruptCheckbox.checked = localStorage.getItem('allow_interrupt') !== 'false';
        if (voiceSelect) voiceSelect.value = localStorage.getItem('selected_voice') || 'fr-FR-DeniseNeural';
        
        // ‚úÖ ATTACHER LES √âV√âNEMENTS DE SAUVEGARDE
        if (groqApiKeyInput) {
            groqApiKeyInput.oninput = function() {
                console.log('üî• GROQ TAPE:', this.value);
                localStorage.setItem('groq_api_key', this.value);
            };
        }
        
        if (openaiApiKeyInput) {
            openaiApiKeyInput.oninput = function() {
                console.log('üî• OPENAI TAPE:', this.value);
                localStorage.setItem('openai_api_key', this.value);
            };
        }
        
        if (autoSpeakCheckbox) {
            autoSpeakCheckbox.onchange = () => localStorage.setItem('auto_speak', autoSpeakCheckbox.checked);
        }
        
        if (allowInterruptCheckbox) {
            allowInterruptCheckbox.onchange = () => localStorage.setItem('allow_interrupt', allowInterruptCheckbox.checked);
        }
        
        if (voiceSelect) {
            voiceSelect.onchange = () => localStorage.setItem('selected_voice', voiceSelect.value);
        }
        
        // ‚úÖ ATTACHER LES √âV√âNEMENTS DES BOUTONS
       // REMPLACER CES LIGNES :
// if (startButton) startButton.onclick = start;
// if (stopButton) stopButton.onclick = stop;

// PAR CETTE NOUVELLE LIGNE :
if (toggleButton) toggleButton.onclick = toggleRecording;
        if (clearButton) clearButton.onclick = clearConversation;
        if (sendButton) sendButton.onclick = handleTextInput;
        
        if (textInput) {
            textInput.addEventListener('keydown', (event) => {
                if (event.key === 'Enter') {
                    event.preventDefault();
                    handleTextInput();
                }
            });
        }
        
        // ‚úÖ TOGGLE PARAM√àTRES
        const settingsToggle = document.getElementById('settingsToggle');
        if (settingsToggle) settingsToggle.onclick = toggleSettings;
        
        log('üéØ Syst√®me complet initialis√© avec succ√®s!', 'success');
        console.log('=== VALEURS CHARG√âES ===');
        console.log('Groq:', groqApiKeyInput?.value ? 'Configur√©' : 'Vide');
        console.log('OpenAI:', openaiApiKeyInput?.value ? 'Configur√©' : 'Vide');

        // Nettoyage au d√©chargement
        window.addEventListener('beforeunload', () => {
            stop();
            for (const [key, url] of ttsCache) {
                URL.revokeObjectURL(url);
            }
            ttsCache.clear();
        });
    };
</script>
<!-- ‚ú® DRAWER - Ajoute √ßa juste AVANT la fermeture du body -->
<div id="drawer" class="fixed inset-y-0 right-0 z-50 w-80 bg-gradient-to-b from-[#1e3c72] to-[#2a5298] shadow-xl transform translate-x-full transition-transform duration-300 ease-in-out">
    <!-- Header du drawer -->
    <div class="flex items-center justify-between p-4 border-b border-white/20">
        <h2 class="text-white text-lg font-semibold">‚öôÔ∏è Panneau de contr√¥le</h2>
        <button id="drawerClose" class="text-white/70 hover:text-white transition-colors">
            <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" />
            </svg>
        </button>
    </div>
    
    <!-- Contenu du drawer -->
    <div class="p-4 space-y-4 overflow-y-auto h-full pb-20">
        
        <!-- Section Param√®tres d√©taill√©s -->
        <div class="bg-white/10 backdrop-blur-xl rounded-xl p-4 border border-white/20">
            <h3 class="text-white font-medium mb-3">üîß Param√®tres avanc√©s</h3>
            
            <!-- Copie tes param√®tres ici depuis settingsContent -->
            <input type="text" id="drawerGroqKey" placeholder="Cl√© API Groq (STT)" class="w-full bg-white/10 border border-white/20 rounded-lg px-3 py-2 text-white text-sm placeholder-white/60 focus:outline-none focus:border-green-400/50 focus:bg-white/15 mb-3">
            <input type="text" id="drawerOpenaiKey" placeholder="Cl√© API OpenAI (LLM)" class="w-full bg-white/10 border border-white/20 rounded-lg px-3 py-2 text-white text-sm placeholder-white/60 focus:outline-none focus:border-green-400/50 focus:bg-white/15 mb-3">
            
            <div class="grid grid-cols-2 gap-2 text-xs">
                <div class="flex items-center gap-1">
                    <span class="text-white">Auto TTS</span>
                    <label class="relative inline-block w-[30px] h-[18px]">
                        <input type="checkbox" id="drawerAutoSpeak" class="opacity-0 w-0 h-0 peer" checked>
                        <span class="absolute cursor-pointer top-0 left-0 right-0 bottom-0 bg-white/30 rounded-full transition-colors peer-checked:bg-green-400 before:absolute before:content-[''] before:h-[14px] before:w-[14px] before:left-[2px] before:bottom-[2px] before:bg-white before:rounded-full before:transition-transform peer-checked:before:translate-x-3"></span>
                    </label>
                </div>
                <div class="flex items-center gap-1">
                    <span class="text-white">Interrupt</span>
                    <label class="relative inline-block w-[30px] h-[18px]">
                        <input type="checkbox" id="drawerAllowInterrupt" class="opacity-0 w-0 h-0 peer" checked>
                        <span class="absolute cursor-pointer top-0 left-0 right-0 bottom-0 bg-white/30 rounded-full transition-colors peer-checked:bg-green-400 before:absolute before:content-[''] before:h-[14px] before:w-[14px] before:left-[2px] before:bottom-[2px] before:bg-white before:rounded-full before:transition-transform peer-checked:before:translate-x-3"></span>
                    </label>
                </div>
            </div>
            
            <select id="drawerVoiceSelect" class="w-full mt-2 bg-white/10 border border-white/20 rounded-lg px-3 py-2 text-white text-sm appearance-none">
                <option value="fr-FR-DeniseNeural">Denise</option>
                <option value="fr-FR-EloiseNeural">Eloise</option>
                <option value="fr-FR-FabriceNeural">Fabrice</option>
                <option value="fr-FR-HenriNeural">Henri</option>
            </select>
        </div>
        
        <!-- Section Statistiques -->
        <div class="bg-white/10 backdrop-blur-xl rounded-xl p-4 border border-white/20">
            <h3 class="text-white font-medium mb-3">üìä Statistiques</h3>
            <div class="space-y-2 text-sm text-white/80">
                <div class="flex justify-between">
                    <span>Cache hits:</span>
                    <span id="drawerCacheHits" class="text-green-400">0</span>
                </div>
                <div class="flex justify-between">
                    <span>Latence:</span>
                    <span id="drawerLatency" class="text-blue-400">0ms</span>
                </div>
                <div class="flex justify-between">
                    <span>Qualit√© audio:</span>
                    <span id="drawerQuality" class="text-yellow-400">0%</span>
                </div>
                <div class="flex justify-between">
                    <span>Messages:</span>
                    <span id="drawerMessageCount" class="text-purple-400">0</span>
                </div>
            </div>
        </div>
        
        <!-- Section Actions rapides -->
        <div class="bg-white/10 backdrop-blur-xl rounded-xl p-4 border border-white/20">
            <h3 class="text-white font-medium mb-3">‚ö° Actions rapides</h3>
            <div class="space-y-2">
                <button id="drawerClearCache" class="w-full py-2 bg-red-500/80 hover:bg-red-500 rounded-lg text-white text-sm transition-colors">
                    üóëÔ∏è Vider le cache
                </button>
                <button id="drawerExportConversation" class="w-full py-2 bg-blue-500/80 hover:bg-blue-500 rounded-lg text-white text-sm transition-colors">
                    üíæ Exporter conversation
                </button>
                <button id="drawerResetSettings" class="w-full py-2 bg-yellow-500/80 hover:bg-yellow-500 rounded-lg text-white text-sm transition-colors">
                    üîÑ Reset param√®tres
                </button>
            </div>
        </div>
        
        <!-- Section Debug √©tendu -->
        <div class="bg-white/10 backdrop-blur-xl rounded-xl p-4 border border-white/20">
            <h3 class="text-white font-medium mb-3">üîç Debug √©tendu</h3>
            <div id="drawerDebug" class="bg-black/20 rounded-lg p-3 text-xs text-white/70 h-32 overflow-y-auto">
                <div>‚ö° Syst√®me pr√™t...</div>
            </div>
        </div>
        
    </div>
</div>

<!-- Overlay pour fermer le drawer -->
<div id="drawerOverlay" class="fixed inset-0 bg-black/50 z-40 opacity-0 pointer-events-none transition-opacity duration-300"></div>
<div id="conversationDrawer" class="fixed inset-y-0 left-0 z-50 w-80 bg-gradient-to-b from-[#1e3c72] to-[#2a5298] transform -translate-x-full transition-transform">
    <!-- Ajoute √ßa au d√©but du conversationDrawer -->
<div class="flex items-center justify-between p-4 border-b border-white/20">
    <h2 class="text-white text-lg font-semibold">üí¨ Conversations</h2>
    <button id="conversationDrawerClose" class="text-white/70 hover:text-white">
        ‚úï
    </button>
</div>
    <div id="conversationHistory" class="p-4 overflow-y-auto h-full">
        <!-- Historique des conversations ici -->
    </div>
</div>
</body><!-- Navigation Component -->
<script src="/components/nav.js"></script>
</html>